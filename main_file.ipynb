{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "743f63f0",
   "metadata": {},
   "source": [
    "This notebook was used to create a small transformer model and develop my own tokenizer needed for the task of predicting the next symbol in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-23T20:51:27.214191Z",
     "start_time": "2024-10-23T20:51:27.212217Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from collections import defaultdict, Counter\n",
    "import torch.nn.functional as F\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e8ea97",
   "metadata": {},
   "source": [
    "Data processing and tokenizer development part\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "248f10d7890357a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-23T20:51:27.517195Z",
     "start_time": "2024-10-23T20:51:27.216195Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_parquet('props-proofs.parquet').head(110)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7fb3b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['combined'] = df['proposition'] + ' ' + df['proof']\n",
    "df.drop(columns=['proposition', 'proof', 'imports', 'filename', 'symbolic_name'], inplace=True)\n",
    "df_test = df.iloc[100:]\n",
    "df = df.iloc[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f1fec206269c67ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-23T20:51:39.855315Z",
     "start_time": "2024-10-23T20:51:39.852310Z"
    }
   },
   "outputs": [],
   "source": [
    "def find_unique_symbols(df, column_name):\n",
    "    \"\"\"\n",
    "    This function finds all unique symbols occuring in our dataset and changes it to a set. \n",
    "    \"\"\"\n",
    "    all_text = ''.join(df[column_name].dropna().astype(str))\n",
    "    unique_symbols = set(all_text)\n",
    "    \n",
    "    return unique_symbols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1cacd3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_tokenizer(symbol_set):\n",
    "    \"\"\"\n",
    "    This function creates a tokenizer that changes a string into words, symbols, and whitespace tokens.\n",
    "    An exception is added for variable names so that they are not split into individual characters. i.e. 'var_name' is one token instead of 'var' '_' 'name'.\n",
    "\n",
    "    \"\"\"\n",
    "    filtered_symbols = {s for s in symbol_set if not s.isalpha()}\n",
    "\n",
    "    symbol_pattern = '|'.join(re.escape(s) for s in sorted(filtered_symbols, key=len, reverse=True))\n",
    "\n",
    "    def tokenizer(input_string):\n",
    "        tokens = re.findall(rf'\\b\\w+(?:_\\w+)*\\b|{symbol_pattern}|\\s|[^\\w\\s]', input_string)\n",
    "        return tokens\n",
    "\n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "symbol_set = find_unique_symbols(df, 'combined')\n",
    "tokenizer = create_tokenizer(symbol_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "63c5ce44",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.apply(lambda x: tokenizer(x['combined']), axis=1).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e8492c",
   "metadata": {},
   "source": [
    "Model development part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "419e1233",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SzymonWo≈∫niak(272471\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25, Loss: 4.294771194458008\n",
      "Epoch 2/25, Loss: 3.3761261105537415\n",
      "Epoch 3/25, Loss: 3.1697906851768494\n",
      "Epoch 4/25, Loss: 2.9279643297195435\n",
      "Epoch 5/25, Loss: 2.71120285987854\n",
      "Epoch 6/25, Loss: 2.5502926111221313\n",
      "Epoch 7/25, Loss: 2.445933520793915\n",
      "Epoch 8/25, Loss: 2.3825116753578186\n",
      "Epoch 9/25, Loss: 2.3142624497413635\n",
      "Epoch 10/25, Loss: 2.2764819264411926\n",
      "Epoch 11/25, Loss: 2.146627187728882\n",
      "Epoch 12/25, Loss: 2.090774178504944\n",
      "Epoch 13/25, Loss: 1.9795898497104645\n",
      "Epoch 14/25, Loss: 1.9832974970340729\n",
      "Epoch 15/25, Loss: 1.8855700194835663\n",
      "Epoch 16/25, Loss: 1.8477830588817596\n",
      "Epoch 17/25, Loss: 1.8249483406543732\n",
      "Epoch 18/25, Loss: 1.759735107421875\n",
      "Epoch 19/25, Loss: 1.685219556093216\n",
      "Epoch 20/25, Loss: 1.683346927165985\n",
      "Epoch 21/25, Loss: 1.700977474451065\n",
      "Epoch 22/25, Loss: 1.6306636333465576\n",
      "Epoch 23/25, Loss: 1.512267678976059\n",
      "Epoch 24/25, Loss: 1.5150374472141266\n",
      "Epoch 25/25, Loss: 1.491888552904129\n",
      "Model saved to transformer_model.pth\n"
     ]
    }
   ],
   "source": [
    "class TokenDataset(Dataset):\n",
    "    \"\"\"\n",
    "    This class created the dataset for the model from our given cleaned and tokenized data.\n",
    "    \"\"\"\n",
    "    def __init__(self, data, vocab=None, max_vocab_size=30000):\n",
    "        self.data = data\n",
    "        \n",
    "        if vocab is None:\n",
    "            token_counts = Counter(token for line in data for token in line)\n",
    "            most_common_tokens = token_counts.most_common(max_vocab_size - 2)\n",
    "            self.vocab = {token: idx + 2 for idx, (token, _) in enumerate(most_common_tokens)}\n",
    "            self.vocab['<PAD>'] = 0\n",
    "            self.vocab['<UNK>'] = 1\n",
    "        else:\n",
    "            self.vocab = vocab\n",
    "\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        self.pad_token = self.vocab['<PAD>']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokens = self.data[idx]\n",
    "        # before the token text\n",
    "        input_tokens = tokens[:-1] \n",
    "        # token to predict\n",
    "        target_token = tokens[1:] \n",
    "        \n",
    "        input_indices = [self.vocab.get(token, self.vocab['<UNK>']) for token in input_tokens]\n",
    "        target_indices = [self.vocab.get(token, self.vocab['<UNK>']) for token in target_token]\n",
    "\n",
    "        return torch.tensor(input_indices, dtype=torch.long), torch.tensor(target_indices, dtype=torch.long)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    This function prepares the batches for the model.\n",
    "    \"\"\"\n",
    "    inputs, targets = zip(*batch)\n",
    "    inputs_padded = nn.utils.rnn.pad_sequence(inputs, batch_first=True, padding_value=0)\n",
    "    targets_padded = nn.utils.rnn.pad_sequence(targets, batch_first=True, padding_value=0)\n",
    "    return inputs_padded, targets_padded\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    \"\"\"\n",
    "    This is our transformer model class\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embedding_dim=512, nhead=8, num_layers=6, dim_feedforward=2048, dropout=0.1):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=embedding_dim,\n",
    "            nhead=nhead,\n",
    "            num_encoder_layers=num_layers,\n",
    "            num_decoder_layers=num_layers,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        self.fc_out = nn.Linear(embedding_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n",
    "        src_emb = self.embedding(src)  # [seq_length, batch_size, embedding_dim]\n",
    "        tgt_emb = self.embedding(tgt)\n",
    "        \n",
    "        transformer_output = self.transformer(src_emb, tgt_emb, src_mask=src_mask, tgt_mask=tgt_mask)\n",
    "        output = self.fc_out(transformer_output)  # [seq_length, batch_size, vocab_size]\n",
    "        \n",
    "        return output\n",
    "\n",
    "def generate_square_subsequent_mask(size):\n",
    "    \"\"\"\n",
    "    This function ensures that the model does not look into the future tokens.\n",
    "    \"\"\"\n",
    "    mask = torch.triu(torch.ones(size, size) * float('-inf'), diagonal=1)\n",
    "    return mask\n",
    "\n",
    "\n",
    "def train_model(model, dataloader, criterion, optimizer, epochs=5):\n",
    "    \"\"\"\n",
    "    This function trains the given model using the given dataloader, criterion, and optimizer.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for src, tgt in dataloader:\n",
    "            src, tgt = src.transpose(0, 1), tgt.transpose(0, 1)\n",
    "            \n",
    "            src_mask = generate_square_subsequent_mask(src.size(0))\n",
    "            tgt_mask = generate_square_subsequent_mask(tgt.size(0) - 1)\n",
    "            \n",
    "            output = model(src, tgt[:-1], src_mask=src_mask, tgt_mask=tgt_mask)\n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            output = output.reshape(-1, output_dim)\n",
    "            tgt = tgt[1:].reshape(-1)\n",
    "            loss = criterion(output, tgt)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(dataloader)}\")\n",
    "\n",
    "\n",
    "dataset = TokenDataset(data, max_vocab_size=30000)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "embedding_dim = 512\n",
    "model = TransformerModel(vocab_size=dataset.vocab_size, embedding_dim=embedding_dim, num_layers=3)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=dataset.pad_token)\n",
    "\n",
    "train_model(model, dataloader, criterion, optimizer, 25)\n",
    "model_path = 'transformer_model.pth'\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(f'Model saved to {model_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fc25999d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted token: '.'\n"
     ]
    }
   ],
   "source": [
    "def predict_next_token(model, sequence, vocab, max_len=50):\n",
    "    \"\"\"\n",
    "    This function predicts the next token based on the given sequence.\n",
    "    \n",
    "    Inputs:\n",
    "    - model: trained transformer model\n",
    "    - sequence: base sequence before the predictied token\n",
    "    - vocab: dictionalry mapping tokens to indexes\n",
    "    - max_len: maximum amount of tokens to generate\n",
    "    \n",
    "    Outputs\n",
    "    - Next given tokens\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    input_indices = [vocab.get(token, vocab['<UNK>']) for token in sequence]\n",
    "    input_tensor = torch.tensor(input_indices, dtype=torch.long).unsqueeze(1)\n",
    "    src_mask = generate_square_subsequent_mask(input_tensor.size(0))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor, input_tensor, src_mask=src_mask, tgt_mask=src_mask)\n",
    "        output = output[-1, 0, :]\n",
    "        \n",
    "        probabilities = F.softmax(output, dim=-1)\n",
    "        \n",
    "        # most probable next token is chosen\n",
    "        predicted_index = torch.argmax(probabilities).item()\n",
    "        # print(predicted_index)\n",
    "        idx_to_token = {idx: token for token, idx in vocab.items()}\n",
    "        # print(idx_to_token)\n",
    "        predicted_token = idx_to_token.get(predicted_index, '<UNK>')\n",
    "    \n",
    "    return predicted_token\n",
    "test_sequence = ['Lemma', ' ', 'scons_comp', ' ', 'x', ' ', 'f', ' ', 'g', ' ', ':', ' ', '(', 'x', ' ', '.', ':', ' ', 'f', ')', ' ', '>', '>', '>', ' ', 'g', ' ', '=', ' ', '(', 'g', ' ', 'x', ')', ' ', '.', ':', ' ', 'f', ' ', '>', '>', '>', ' ', 'g', '.', ' ', 'Proof', '.', ' ', 'f_ext', ';', ' ', 'let', ' ', 'x', ' ', ':', '=', ' ', 'fresh', ' ', 'in', ' ', 'intros', ' ', 'x', ';', ' ', 'now', ' ', 'destruct', ' ', 'x', '.', ' ', 'Qed']\n",
    "\n",
    "\n",
    "predicted_token = predict_next_token(model, test_sequence, dataset.vocab)\n",
    "print(\"Predicted token: '\" + predicted_token+\"'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb6ea5f",
   "metadata": {},
   "source": [
    "Model testing part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "08e5d693",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "data_test = df_test.apply(lambda x: tokenizer(x['combined']), axis=1).tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2332239",
   "metadata": {},
   "source": [
    "Tests token lists were taken from data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "567265a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted tokens for transformer model:\n",
      "' '\n",
      "Correct token:\n",
      "' '\n",
      "Predicted tokens for transformer model:\n",
      "' '\n",
      "Correct token:\n",
      "'&'\n",
      "Predicted tokens for transformer model:\n",
      "'>'\n",
      "Correct token:\n",
      "'>'\n"
     ]
    }
   ],
   "source": [
    "tests_og = [\n",
    "['Lemma', ' ', 'p_impl_p', ' ', ':', ' ', 'forall', ' ', '(', 'p', ':', 'Prop', ')', ',', ' ', 'p', ' ', '/', '\\\\', ' ', 'True', ' ', '-', '>', ' ', 'p', '.', ' ', 'Proof', '.', ' ', 'intros', '.', ' ', 'itauto', ' ', 'idtac', '.', ' ', 'Qed', '.'],\n",
    "['Lemma', ' ', 'x_andb_true', ' ', ':', ' ', 'forall', ' ', 'x', ',', ' ', 'Is_true', ' ', '(', 'x', ' ', '&', '&', ' ', 'true', ')', ' ', '-', '>', ' ', 'Is_true', ' ', 'x', '.', ' ', 'Proof', '.', ' ', 'intros', '.', ' ', 'itauto', ' ', 'idtac', '.', ' ', 'Qed', '.'],\n",
    "['Lemma', ' ', 'l2', ' ', ':', ' ', 'forall', ' ', '(', 'x', ':', 'Z', ')', ',', ' ', 'x', ' ', '>', '=', ' ', '0', ' ', '-', '>', ' ', 'x', ' ', '<', '=', ' ', '0', ' ', '-', '>', ' ', 'x', ' ', '<', '>', ' ', '0', ' ', '-', '>', ' ', 'False', '.', ' ', 'Proof', '.', ' ', 'intros', '.', ' ', 'itauto', ' ', 'lia', '.', ' ', 'Qed', '.'],\n",
    "]\n",
    "tests = [\n",
    "['Lemma', ' ', 'p_impl_p', ' ', ':', ' ', 'forall', ' ', '(', 'p', ':', 'Prop', ')', ',', ' ', 'p', ' ', '/', '\\\\'],\n",
    "['Lemma', ' ', 'x_andb_true', ' ', ':', ' ', 'forall', ' ', 'x', ',', ' ', 'Is_true', ' ', '(', 'x', ' ', '&'],\n",
    "['Lemma', ' ', 'l2', ' ', ':', ' ', 'forall', ' ', '(', 'x', ':', 'Z', ')', ',', ' ', 'x', ' ', '>', '=', ' ', '0', ' ', '-', '>', ' ', 'x', ' ', '<', '=', ' ', '0', ' ', '-']\n",
    "]\n",
    "for i in range(3):\n",
    "    print('Predicted tokens for transformer model:')\n",
    "    print(\"'\" + predict_next_token(model, tests[i], dataset.vocab)+ \"'\")\n",
    "    print('Correct token:')\n",
    "    x = len(tests[i])\n",
    "    print(\"'\"+ tests_og[i][x]+\"'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
